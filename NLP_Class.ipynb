{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyML7kmOSsCmDkCRhAZhUe9b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arsilla/TextMining_Tias/blob/main/NLP_Class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1b9bILMLyFY"
      },
      "source": [
        "# Run these cells first!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cT7xwOCL2ZR"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeCmXIYeKjUI"
      },
      "source": [
        "from transformers import DistilBertConfig, TFDistilBertModel\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "\n",
        "\n",
        "def get_bert_model(n_labels):\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "    distil_bert = 'distilbert-base-uncased'\n",
        "\n",
        "\n",
        "    config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "    config.output_hidden_states = False\n",
        "    transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)\n",
        "\n",
        "    input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
        "    input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
        "\n",
        "    embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "    X = tf.keras.layers.Dense(50, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dropout(0.2)(X)\n",
        "    X = tf.keras.layers.Dense(int(n_labels), activation='sigmoid')(X)\n",
        "    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "    for layer in model.layers[:3]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "    return model\n",
        "\n",
        "def bert_tokenize(sentences, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [],[],[]\n",
        "    for sentence in tqdm(sentences):\n",
        "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=128, padding='max_length', \n",
        "                                             return_attention_mask=True, return_token_type_ids=True, truncation=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])        \n",
        "        \n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksog6asks4RB"
      },
      "source": [
        "# Text Mining exercises\n",
        "\n",
        "For this excercise, you will try to do create a similar result for the Sustainability Case. In this case we created:\n",
        "\n",
        "- a topic model\n",
        "- A sentiment classifier using BERT\n",
        "\n",
        "However, although all the data is publicly available, it would require a lot of downloading, extracting and pre-processing to work. In addition the sentiment labels are not publicly available.\n",
        "\n",
        "So we will use two different datasets instead:\n",
        "\n",
        "- **Sentiment140 dataset**\n",
        "\n",
        "    A twitter dataset with 1.6 million tweets containing sentiment labels.\n",
        "\n",
        "Run the first cell to download the data, then read from the folder 'data' using `pandas`.\n",
        "\n",
        "- **Sklearn Newsgroups dataset**\n",
        "use:\n",
        "\n",
        "``` python\n",
        "from sklearn import datasets\n",
        "newsgroups = datasets.fetch_20newsgroups()\n",
        "```\n",
        "\n",
        "We'll be applying our analysis on the newsgroups data, but using the twitter dataset to train a sentiment classifier, which we will apply to the newsgroups data.\n",
        "In addition, we will be using DistilBERT (a 'distilled' version of BERT) for the sentiment part, as it will be faster to train.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guvm7J8uwogD"
      },
      "source": [
        "# Exercise 0: Retrieve both datasets and explore it's contents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU4A9rHcxJlr"
      },
      "source": [
        "# Run this cell to download and unzip the sentiment data\n",
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "!unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpekAai_sw6n"
      },
      "source": [
        "from sklearn import datasets\n",
        "N = 5000 # We're limiting the dataset, because trainign on 1.6 mil tweets takes too long\n",
        "newsgroups = datasets.fetch_20newsgroups()\n",
        "df_sentiment = pd.read_csv('data/training.1600000.processed.noemoticon.csv', \n",
        "                           names=['sentiment', 'id', 'date', 'query', 'user', 'text'],\n",
        "                           encoding='latin-1')\n",
        "\n",
        "df_sent1 = df_sentiment.iloc[:N]\n",
        "df_sent2 = df_sentiment.iloc[-N:]\n",
        "df_sentiment = pd.concat([df_sent1, df_sent2])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hjsDRq5w1CT"
      },
      "source": [
        "# Exercise 1: Clean newsgroups data\n",
        "\n",
        "If you have looked at the Newsgroup data, you might notice that the data has a lot of unecessary extra information, such as the header of the message.\n",
        "\n",
        "We're only interested in the message itself. Write a regular expression that can remove the header information from the newsgroups, and apply it to the data\n",
        "\n",
        "Header example:\n",
        "\n",
        "``` \n",
        "From: ravin@eecg.toronto.edu (Govindan Ravindran)\n",
        "Subject: decoupling caps - onboard\n",
        "Organization: Department of Electrical and Computer Engineering, University of Toronto\n",
        "Lines: 10\n",
        "```\n",
        "\n",
        "HINT: Try to examine a few examples and see what they have in common at the start and the beginning of each header. \n",
        "Also look at a RegEx cheatsheet (like <a href='https://i.stack.imgur.com/KiaKd.png'> this one</a>)\n",
        "\n",
        "HINT 2: If you're having trouble with newlines (`\\n`), try removing them before applying the RegEx pattern, or using the group `[\\W\\w]` instead of `.`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lGBK7_jw0EP"
      },
      "source": [
        "import re\n",
        "pattern = r\"\"\n",
        "pattern = r\"(from)([\\W\\w]*)(lines:\\s\\d{2,})\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrBatV7g0Kv6"
      },
      "source": [
        " # re.IGNORECASE will ignore capital letters in the text when applying the pattern\n",
        " # You can also use .lower on text data, but be aware that this changes all letters to lower case\n",
        "newsgroup_data = []\n",
        "for data in newsgroups.data:\n",
        "    new_text = re.sub(pattern,repl=\"\", string=data, flags=re.IGNORECASE)\n",
        "    newsgroup_data.append(new_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xamlsCeu0j2B",
        "outputId": "47790813-b10e-4f4c-9165-d2b897c1e0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "newsgroup_data[106]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n(posted for a friend)\\nhello there,\\n        I would like to know if any one had any experience with having\\non-board decoupling capacitors (inside a cmos chip) for the power\\nlines. Say I have a lot of space left im my pad limited design.\\nany data on the effect of oxide breakdown? any info or pointers\\nare appreciated.\\n\\nrs\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFtdINT31Wk-"
      },
      "source": [
        "## Exercise 1b: Tokenize the newsgroup_data\n",
        "\n",
        "Using a tokenizer from `nltk`, tokenize the newsgroup_data into a new object called `newsgroup_tokens`. We want to end up with a list of lists, where each sublist contains the tokens of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLlWvcpl8T18"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups = fetch_20newsgroups()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVdf5PKt1ppz"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tok = RegexpTokenizer(pattern=r'[a-zA-Z]{2,}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_BWkKPz1scQ"
      },
      "source": [
        "# Tokenize your data here\n",
        "# You can use a for loop similar to Exercise 1\n",
        "for "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tshxXky8416I"
      },
      "source": [
        "# Exercise 2: Create an LDA model\n",
        "\n",
        "a) Load the `LdaModel` from `gensim.models` and `Dictionary` from `gensim.corpora`.\n",
        "\n",
        "b) Create a dictionary instance using the `Dictionary` class from gensim, with your tokens as input (the list of lists)\n",
        "\n",
        "c) Build a corpus by calling the `doc2bow` function on the dictionary you created, with each list of tokens in your `tokens` list as input, and put them in a list.\n",
        "\n",
        "d) Create and train an LDA model, by calling the LdaModel with as input your corpus. Set the id2word as your dictionary.\n",
        "Think about what settings you want to use for *alpha* and *eta* to control the number of topics per document, and words per topic respectivally. \n",
        "\n",
        "e) *(Optional):* Look at your results and see if your satisfied. If not, try to think what other ways you can improve the model by filtering out certain words (look at the `filter_extremes` function on the dictionary for example) or remove stopwords from your tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9gug9oIF0Qp"
      },
      "source": [
        "## Load stopwords if you want them\n",
        "## Uncomment to use\n",
        "\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# try:\n",
        "#     input_stopwords = stopwords.words('english')\n",
        "# except:\n",
        "#     nltk.download('stopwords')\n",
        "#     input_stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsrJ0jqwE60d"
      },
      "source": [
        "lda = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhplffDaF06r"
      },
      "source": [
        "When your satisfied with your LDA model, save it for later (just don't overwrite the name with someting else).\n",
        "\n",
        "We're now going to train a BERT model.\n",
        "\n",
        "\n",
        "# BERT Sentiment analysis\n",
        "For ease, we'll be using the huggingface transformers instance of DistilBERT, which is a lighter version of the full BERT model.\n",
        "\n",
        "They have a pre-trained version (for english) available that can be used for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlLSReTbG7dO"
      },
      "source": [
        "Before we start training the model, we're going to clean up the sentiment dataset.\n",
        "\n",
        "## Exercise 3: Clean up sentiment dataset\n",
        "\n",
        "a) The sentiment labels are currenty 0 for negative, 4 for positive. Change the values to 0 (negative) and 1 (positive) for easier input to the model.\n",
        "    hint: you can do this in differnt ways, but using a dictionary is one way:\n",
        "\n",
        "``` python\n",
        ">>> df.loc[0,'column_to_change']\n",
        " 0    a\n",
        " 1    b\n",
        " 2    a\n",
        " Name: column_to_change, dtype: object\n",
        ">>> df['column_to_change'] = df['column_to_change'].replace({'a': 1})\n",
        ">>> df['column_to_change']\n",
        " 0    1\n",
        " 1    b\n",
        " 2    1\n",
        " Name: column_to_change, dtype: object\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2ba_c8aJLXB"
      },
      "source": [
        "## Your answer here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3tAgRHIJ-fh"
      },
      "source": [
        "b) Remove any columns that are not necessary, and only keep the 'sentiment' and 'text' columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRhubnJfKNvh"
      },
      "source": [
        "# your answer here\n",
        "df = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23W7DBMrKQkG"
      },
      "source": [
        "c) (Optional) Remove any unwanted characters from the text column. Note that this is not *necessary* for the BERT tokenizer to work (it can handle any input basically). However you might consider removing twitter handles or URLS, simply because they are not relevant for the analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckcNaHKSKhZc"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyH29pOcKkCa"
      },
      "source": [
        "## Exercise 4: Initialize DistilBERT and prepare train and test set\n",
        "\n",
        "For the purpose of this exercise, I've prepared a version of the DistilBERT model (based on <a href='https://github.com/huggingface/transformers'>one of these</a>) that you can use for fine-tuning (transfer learning) a model for classification.\n",
        "\n",
        "You can get this model by calling the `get_bert_model` function below.\n",
        "\n",
        "## NOTE: Make double sure your runtime is set to GPU, otherwise one epoch might take 30 minutes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw1YfsEXMhH9"
      },
      "source": [
        "from transformers import DistilBertTokenizer, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = get_bert_model(2)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ0NohTNPNVV"
      },
      "source": [
        "The model expects two inputs, the tokens (as a one-hot encoded vector) and the masked tokens. \n",
        "The input is handled by the tokenizer, and you can use the `bert_tokenize` function to get the input transformed for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3hXGdRyQ1NW"
      },
      "source": [
        "sentences = ['testing this thing', \n",
        "             'short sentences will be padded with zeros, and long ones are cut off to make sure everything is the same size']\n",
        "tokenized = bert_tokenize(sentences, tokenizer)\n",
        "tokenized"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBx-DBl-SoAm"
      },
      "source": [
        "Now we need to prepare the data for training.\n",
        "\n",
        "a) Make two arrays of text (tweets) and labels (sentiment) called X and Y, then use the `train_test_split` from sklearn to split up the data twice. Once in `train` and `test`, and then the `test` set again in `test` and `val` (validate). Think about what percentage of the sets you want to assing to each set, and use the `test_size` parameter for that, as a float between 0-1, where `1=100%`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3keNF2KTLxB"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "## FILL THIS IN\n",
        "X = \n",
        "\n",
        "Y = \n",
        "\n",
        "X_train, X_test, y_train,  y_test = train_test_split()\n",
        "X_test, X_val, y_test, y_val = train_test_split()\n",
        "\n",
        "print(f\"Train set size.    : {len(X_train)} tweets, {(len(X_train)/len(X))*100}% of total size\")\n",
        "print(f\"Test set size.     : {len(X_test)} tweets, {(len(X_test)/len(X))*100}% of total size\")\n",
        "print(f\"Validation set size: {len(X_val)} tweets, {(len(X_val)/len(X))*100}% of total size\")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCb3SkNW59l5"
      },
      "source": [
        "# Checking we have good even distributions of labels for all sets\n",
        "print(np.unique(y_train, return_counts=True))\n",
        "print(np.unique(y_val, return_counts=True))\n",
        "print(np.unique(y_test, return_counts=True))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6p1qyN9K0NP"
      },
      "source": [
        "Make sure the input text (`X_train`, `X_val` and `X_test`) have been tokenized for input into the model, using the `bert_tokenize` function!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4pGPSepf8Vz"
      },
      "source": [
        ""
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTSDrzZAVPVR"
      },
      "source": [
        "b) Now train the model on the train set. You can do this by calling the model.train() function on the model object.\n",
        "Use a batch size of 32 and start with a few epochs. See how the model is performing.\n",
        "\n",
        "Running the training may take a few minutes per epoch, so perhaps this is a good moment for some coffee."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgWqM9KNPMo0"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=10)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCP6AQ48CysH"
      },
      "source": [
        "c) Make predictions on the test set (`X_test`) and compare them to the true labels (`y_test`), what is the accuracy?. Get the confusion matrix from `sklearn.metrics` to see where the model is making mistakes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFRrD7ItCw1W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKGDDO3wD3Nk"
      },
      "source": [
        "d) Also try to get a few examples of wrong labels, where is it going wrong and do you know why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKJGv5ZfD8c1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmyAK8pyD84K"
      },
      "source": [
        "## Exercise 5: Combine results\n",
        "\n",
        "We're now going to apply both models (LDA and Sentiment) to the newsgroup data.\n",
        "\n",
        "The sentiment model might not perform very well on the newsgroup data, since it's a different dataset. However, we hope that the language is similar enough that the model can generalize to the newsgroups data as well.\n",
        "\n",
        "a-1) Tokenize the newsgroups data sentences (called `newsgroup_data`) using the `bert_tokenize` function. Create predictions from the tokenized input and save as `y_sent_proba`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGYQOoYYErbk"
      },
      "source": [
        ""
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW3WRd8mF_ow"
      },
      "source": [
        "a-2) The output of the `y_sent_proba` is of shape `(N, 2)` where N is the number of samples in newsgroups_data.\n",
        "The two numbers given to each sample, are the confidence of that newsgroup beloging to class 0 (negative) or 1 (positive) by index.\n",
        "\n",
        "So if `newsgroup_data[1]` has prediction `[0.3, 0.7]` the model is 70% confident that the sentiment is positive.\n",
        "\n",
        "To get just a label back, we can get the maximum value of each of these samples by using `np.argmax(y_sent_proba, 1)`. \n",
        "\n",
        "Convert `y_sent_proba` to an array of intergers (labels) using np.argmax and save it as `y_sentiment`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftGxZVBAGAA8",
        "outputId": "3968a5c4-c434-4b12-d47c-b432347dd0ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_sentiment = "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([  930, 10384]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vZlQgkqNHNU",
        "outputId": "a84be31a-1323-4e41-f9ae-389071232eeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3pzQcaKErt_"
      },
      "source": [
        "b) Now have the LDA model run predictions on the newsgroup data per text. You can do this by running `lda.get_document_topics(doc)` for each `doc` in our corpus. (corpus should still exist in our namespace, if not, run the old cells above where we made the LDA model and trained it).\n",
        "\n",
        "Save these under y_topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7uS3-WUFjv2"
      },
      "source": [
        "y_topic = []\n",
        "for"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CxM6veXFpUI"
      },
      "source": [
        "Now we have both sentiment and topic predictions, saved as y_sentiment and y_topic. \n",
        "\n",
        "c) Create a visualisation using both of these values. For example, show a barchart of the number of positive and negative values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oINVTVCIOZ2o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S51HPXyDFoaD"
      },
      "source": [
        ""
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWB_7QFiIQ-g"
      },
      "source": [
        "# Congrats! You're done!\n",
        "\n",
        "You have now succesfully created an LDA model and finetuned DistilBERT for sentiment analysis. \n",
        "\n",
        "Obviously you can experiment a lot more with the output both models, or experiment with the vectorisations . Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8fsVV_aLyOg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}